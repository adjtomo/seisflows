#!/usr/bin/env python3
"""
This Solver module is in charge of interacting with external numerical solvers
such as SPECFEM (2D/3D/3D_GLOBE). This SPECFEM base class provides general
functions that work with all versions of SPECFEM. Subclasses will provide
additional capabilities unique to each version of SPECFEM.

.. note::
    The Base class implementation is almost completely SPECFEM2D related.
    However, SPECFEM2D requires a few unique parameters that 3D/3D_GLOBE
    do not. Because of the inheritance architecture of SeisFlows, we do not
    want the 3D and 3D_GLOBE versions to inherit 2D-specific parameters, so
    we need this this more generalized SPECFEM base class.

TODO
    - add in `apply_hess` functionality that was partially written in legacy code
    - move `_initialize_adjoint_traces` to workflow.migration
    - Add density scaling based on Vp?
"""
import os
import sys
import subprocess
import time
from concurrent.futures import ProcessPoolExecutor, wait
from glob import glob

from seisflows import logger
from seisflows.tools import msg, unix
from seisflows.tools.config import get_task_id, Dict
from seisflows.tools.model import Model
from seisflows.tools.specfem import getpar, setpar, check_source_names


class Specfem:
    """
    Solver SPECFEM [Solver Base]
    ----------------------------
    Defines foundational structure for Specfem-based solver module. 
    Generalized SPECFEM interface to manipulate SPECFEM2D/3D/3D_GLOBE w/ Python

    Parameters
    ----------
    :type syn_data_format: str
    :param syn_data_format: data format for reading synthetic traces into memory.
        Available: ['SU': seismic unix format, 'ASCII': human-readable ascii]
    :type materials: str
    :param materials: Material parameters used to define model. Available:
        ['ELASTIC': Vp, Vs, 'ACOUSTIC': Vp, 'ISOTROPIC', 'ANISOTROPIC']
    :type density: bool
    :param density: How to treat density during inversion. If True, updates
        density during inversion. If False, keeps it constant.
        TODO allow density scaling during an inversion
    :type attenuation: bool
    :param attenuation: How to treat attenuation during inversion.
        if True, turns on attenuation during forward simulations only. If
        False, attenuation is always set to False. Requires underlying
        attenution (Q_mu, Q_kappa) model
    :type smooth_h: float
    :param smooth_h: Gaussian half-width for horizontal smoothing in units
        of meters. If 0., no smoothing applied. Only applicable for workflows:
        ['migration', 'inversion'], ignored for 'forward' workflow.
        SPECFEM3D_GLOBE only: if `smooth_type`=='laplacian' then this is just 
        the X and Y extent of the applied smoothing
    :type smooth_h: float
    :param smooth_v: Gaussian half-width for vertical smoothing in units
        of meters. Only applicable for workflows: ['migration', 'inversion'],
        ignored for 'forward' workflow.
        SPECFEM3D_GLOBE only: if `smooth_type`=='laplacian' then this is just 
        the Z extent of the applied smoothing
    :type components: str
    :param components: components to search for synthetic data with. None by
        default which uses a wildcard when searching for synthetics. If
        provided, User only wants to use a subset of components generated by
        SPECFEM. In that case, `components` should be string of letters such
        as 'ZN' (for up and north components)
    :type solver_io: str
    :param solver_io: format of model/kernel/gradient files expected by the
        numerical solver. Available: ['fortran_binary': default .bin files].
        TODO: ['adios': ADIOS formatted files]
    :type source_prefix: str
    :param source_prefix: prefix of source/event/earthquake files. If None,
        will attempt to guess based on the specific solver chosen.
    :type mpiexec: str
    :param mpiexec: MPI executable used to run parallel processes. Should also
        be defined for the system module

    Paths
    -----
    :type path_data: str
    :param path_data: path to any externally stored waveform data required for 
        data-synthetic comparison
    :type path_specfem_bin: str
    :param path_specfem_bin: path to SPECFEM bin/ directory which
        contains binary executables for running SPECFEM
    :type path_specfem_data: str
    :param path_specfem_data: path to SPECFEM DATA/ directory which must
        contain the CMTSOLUTION, STATIONS and Par_file files used for
        running SPECFEM
    ***
    """
    def __init__(self, syn_data_format="ascii",  materials="acoustic",
                 update_density=False, nproc=1, ntask=1, attenuation=False,
                 smooth_h=0., smooth_v=0., components=None,
                 source_prefix=None, mpiexec=None, workdir=os.getcwd(),
                 path_solver=None, path_eval_grad=None,
                 path_data=None, path_specfem_bin=None, path_specfem_data=None,
                 path_model_init=None, path_model_true=None, path_output=None,
                 **kwargs):
        """
        Set default SPECFEM interface parameters

        .. note::
            Paths listed here are shared with `workflow.forward` and so are not
            included in the class docstring.

        :type workdir: str
        :param workdir: working directory in which to look for data and store
            results. Defaults to current working directory
        :type path_solver: str
        :param path_solver: scratch path for all solver related tasks
        :type path_model_init: str
        :param path_model_init: path to the starting model used to calculate the
            initial misfit. Must match the expected `solver_io` format.
        :type path_model_true: str
        :param path_model_true: path to a target model if `case`=='synthetic' and
            a set of synthetic 'observations' are required for workflow.
        :type path_output: str
        :param path_output: shared output directory on disk for more permanent
            storage of solver related files such as traces, kernels, gradients.
        """
        # Publically accessible parameters
        self.syn_data_format = syn_data_format
        self.materials = materials
        self.nproc = nproc
        self.ntask = ntask
        self.update_density = update_density
        self.attenuation = attenuation
        self.smooth_h = smooth_h
        self.smooth_v = smooth_v
        self.components = components
        self.source_prefix = source_prefix or "SOURCE"

        # Define internally used directory structure
        self.path = Dict(
            scratch=path_solver or os.path.join(workdir, "scratch", "solver"),
            eval_grad=path_eval_grad or
                      os.path.join(workdir, "scratch", "eval_grad"),
            data=path_data or os.path.join(workdir, "SFDATA"),
            output=path_output or os.path.join(workdir, "output"),
            specfem_bin=path_specfem_bin,
            specfem_data=path_specfem_data,
            model_init=path_model_init,
            model_true=path_model_true,
        )
        self.path["mainsolver"] = os.path.join(self.path.scratch, "mainsolver")
        self.path["_solver_output"] = os.path.join(self.path.output, "solver")

        # Private internal parameters for keeping track of solver requirements
        self._parameters = []
        if self.update_density:
            self._parameters.append("rho")

        self._mpiexec = mpiexec
        self._source_names = None  # for property source_names
        self._ext = ""  # for database file extensions

        # Define available choices for check parameters
        self._available_model_types = ["gll"]
        self._available_materials = [
            "ELASTIC", "ACOUSTIC",  # specfem2d, specfem3d
            "ISOTROPIC", "ANISOTROPIC"  # specfem3d_globe
        ]
        # SPECFEM2D specific attributes. Should be overwritten by 3D versions
        self._syn_available_data_formats = ["ASCII", "SU"]
        self._required_binaries = ["xspecfem2D", "xmeshfem2D", "xcombine_sem",
                                   "xsmooth_sem"]
        self._acceptable_source_prefixes = ["SOURCE", "FORCE", "FORCESOLUTION"]

        # Constants that will be referenced during simulations and file I/O.
        # These should be overwritten by all child classes (3D, 3D_GLOBE)
        self._fwd_simulation_executables = ["bin/xmeshfem2D", "bin/xspecfem2D"]
        self._adj_simulation_executables = ["bin/xspecfem2D"]
        self._absorb_wildcard = "absorb_*_*"
        self._forward_array_wildcard = ""

        # Empty variables that will need to be overwritten by SPECFEM3D/3D_GLOBE
        self._regions = None
        self._export_vtk = False

    def check(self):
        """
        Checks parameter validity for SPECFEM input files and model parameters
        """
        assert(self.materials.upper() in self._available_materials), \
            f"solver.materials must be in {self._available_materials}"

        if self.syn_data_format.upper() not in self._syn_available_data_formats:
            raise NotImplementedError(
                f"solver.syn_data_format must be "
                f"{self._syn_available_data_formats}"
            )

        # Check that User has provided appropriate binary files to run SPECFEM
        assert(self.path.specfem_bin is not None and
               os.path.exists(self.path.specfem_bin)), (
            f"`path_specfem_bin` must exist and must point to directory " 
            f"containing SPECFEM executables"
        )
        for fid in self._required_binaries:
            assert(os.path.exists(os.path.join(self.path.specfem_bin, fid))), (
                f"`path_specfem_bin`/{fid} does not exist but is required by "
                f"SeisFlows solver module"
            )

        # Check that SPECFEM/DATA directory exists
        assert(self.path.specfem_data is not None and
               os.path.exists(self.path.specfem_data)), (
            f"`path_specfem_data` must exist and must point to directory " 
            f"containing SPECFEM input files"
        )
        for fid in ["STATIONS", "Par_file"]:
            assert(os.path.exists(os.path.join(self.path.specfem_data, fid))), (
                f"DATA/{fid} does not exist but is required by SeisFlows solver"
            )

        # Make sure source files exist and are appropriately labeled
        assert(self.source_prefix in self._acceptable_source_prefixes), (
            f"SPECFEM `source_prefix` must be in "
            f"{self._acceptable_source_prefixes}"
            )
        assert(glob(os.path.join(self.path.specfem_data,
                                 f"{self.source_prefix}*"))), (
            f"No source files with prefix {self.source_prefix} found in DATA/")

        # Check that model type is set correctly in the Par_file
        model_type = getpar(key="MODEL",
                            file=os.path.join(self.path.specfem_data,
                                              "Par_file"))[1]
        assert(model_type in self._available_model_types), (
            f"SPECFEM Par_file parameter `model`='{model_type}' does not "
            f"match acceptable model types: {self._available_model_types}"
            )

        # Make sure the initial model is set and actually contains files
        assert(self.path.model_init is not None and
               os.path.exists(self.path.model_init)), \
            f"`path_model_init` is required for the solver, but does not exist"

        assert(len(glob(os.path.join(self.path.model_init, "*")))), \
            f"`path_model_init` is empty but should have model files"

        if self.path.model_true is not None:
            assert(os.path.exists(self.path.model_true)), \
                f"`path_model_true` is provided but does not exist"
            assert(len(glob(os.path.join(self.path.model_true, "*")))), \
                f"`path_model_true` is empty but should have model files"

        # Check that the number of tasks/events matches the number of events
        self._source_names = check_source_names(
            path_specfem_data=self.path.specfem_data,
            source_prefix=self.source_prefix, ntask=self.ntask
        )

        assert(isinstance(self.update_density, bool)), \
            f"solver `density` must be True (variable) or False (constant)"

        # Check the size of the DATA/ directory and let the User know if 
        # large files are present, e.g., tomo xyz files or topo/bathy
        for root, dirs, files in os.walk(self.path.specfem_data):
            for name in files:
                fullpath = os.path.join(root, name)
                if not os.path.islink(fullpath):
                    filesize = os.path.getsize(fullpath) / 1E9  # Bytes -> GB
                    if filesize > 0.5:
                        logger.warning(
                            f"SPECFEM DATA/ file '{fullpath}' is >.5GB and "
                            f"will be copied {self.ntask} time(s). Please be "
                            f"sure to check if this file is necessary for your "
                            f"workflow"
                            )

    def setup(self):
        """
        Prepares solver scratch directories for an impending workflow.

        Sets up directory structure expected by SPECFEM and copies or generates
        seismic data to be inverted or migrated.

        Exports INIT/STARTING and TRUE/TARGET models to disk (output/ dir.)
        """
        # Create the internal directory structure required for storing results
        for pathname in ["_solver_output"]:
            unix.mkdir(self.path[pathname])

        # Assign file extensions to be used for database file searching
        model_type = getpar(key="MODEL",
                            file=os.path.join(self.path.specfem_data,
                                              "Par_file"))[1]
        if "gll" in model_type:
            self._ext = ".bin"
        else:
            logger.warning("no SPECFEM model type specified to define file "
                           "extension, defaulting to '.bin'")
            self._ext = ".bin"

        self._initialize_working_directories()
        self._export_starting_models()

    def check_model_values(self, path):
        """
        Convenience function to check parameter and model validity for
        chosen Solver model. Should be called by the Workflow module

        :type path: str
        :param path: path to model file(s) that should be in the format expected
            by the Model class (FORTRAN binary, ADIOS etc.)
        """
        assert os.path.exists(path), f"Model check path does not exist: {path}"

        _model = Model(path=path, parameters=self._parameters,
                       regions=self._regions)
        try:
            _model.check()
            _model.print_stats()
        except AssertionError as e:
            logger.critical(
                msg.cli(str(e), header="model read error", border="=")
            )
            sys.exit(-1)

    def set_parameters(self, keys, vals, file, delim, **kwargs):
        """
        Public API that allows other modules modify solver-specific files with
        paths relative to the `cwd` attribute.

        Primarily used to modify locations or force vector direction for
        the generation of different kernels in noise workflows.

        Only works if file exists, otherwise raises FileNotFoundError
        Kwargs are passed to `seisflows.tools.specfem.setpar()`

        :type key: str
        :param key: case-insensitive key to match in par_file. must match EXACT
        :type val: str
        :param val: value to OVERWRITE to the given key
        :raises FileNotFoundError: if `file` does not exist within the solver's
            working directory
        """
        os.chdir(self.cwd)
        if os.path.exists(file):
            for key, val in zip(keys, vals):
                setpar(key=key, val=val, file=file, delim=delim, **kwargs)
        else:
            raise FileNotFoundError(f"solver/{file} not found, cannot set "
                                    f"parameters")

    @property
    def source_names(self):
        """
        Returns list of source names which should be stored in PAR.SPECFEM_DATA
        Source names are expected to match the following wildcard,
        'PREFIX_*' where PREFIX is something like 'CMTSOLUTION' or 'FORCE'

        .. note::
            Dependent on environment variable 'SEISFLOWS_TASKID' which is
            assigned by system.run() to each individually running process.

        :rtype: list
        :return: list of source names
        """
        if self._source_names is None:
            self._source_names = check_source_names(
                path_specfem_data=self.path.specfem_data,
                source_prefix=self.source_prefix, ntask=self.ntask
            )
        return self._source_names

    @property
    def source_name(self):
        """
        Returns name of source currently under consideration

        .. note::
            Dependent on environment variable 'SEISFLOWS_TASKID' which is
            assigned by system.run() to each individually running process.

        :rtype: str
        :return: given source name for given task id
        """
        return self.source_names[get_task_id()]

    @property
    def cwd(self):
        """
        Returns working directory currently in use by a running solver instance

        .. note::
            Dependent on environment variable 'SEISFLOWS_TASKID' which is
            assigned by system.run() to each individually running process.

        :rtype: str
        :return: current solver working directory
        """
        return os.path.join(self.path.scratch, self.source_name)

    def data_wildcard(self, comp="?"):
        """
        Returns a wildcard identifier for synthetic data based on SPECFEM2D
        file naming schema. Allows formatting dcomponent e.g.,
        when called by solver.data_filenames.

        Some example SPECFEM2D ASCII seismogram file names for reference:
        - AA.S000000.BXY.semd: Membrane wave displacement
        - AA.S000000.BXY.semp: Membrane wave pressure
        - AA.S000000.PRE.semp: P-SV pressure seismogram

        .. note::

            SPECFEM3D/3D_GLOBE versions must overwrite this function

        :type comp: str
        :param comp: component formatter, defaults to wildcard '?'
        :rtype: str
        :return: wildcard identifier for channels
        """
        if self.syn_data_format.upper() == "SU":
            return f"U{comp}*.su"  # e.g., Up_file_single_p.su
        elif self.syn_data_format.upper() == "ASCII":
            return f"*.??{comp}.sem?"  # e.g., AA.S000000.BXY.semd

    def model_wildcard(self, par="*", kernel=False):
        """
        Returns a wildcard identifier to search for models kernels generated by
        the solver. An example SPECFEM2D/3D kernel filename (in 
        FORTRAN binary file format) is: 'proc000001_rho_kernel.bin'
        Whereas the corresponding model would be 'proc000001_rho.bin'

        Allows dynamically searching for specific files when renaming, moving
        or copying files. Also allows for different wildcard for 3D_GLOBE 
        version

        :type par: str
        :param par: parameter formatter, defaults to wildcard '?'
        :type kernel: bool
        :param kernel: wildcarding a kernel file. If True, adds the 'kernel' 
            tag. If not, assuming we are wildcarding for a model file
        :rtype: str
        :return: wildcard identifier for channels
        """
        if kernel:
            _ker = "_kernel"
        else:
            _ker = ""
        return f"proc??????_{par}{_ker}{self._ext}"

    def data_filenames(self, choice="obs"):
        """
        Returns the filenames of SPECFEM2D data, either by the requested
        components or by all available files in the directory.

         .. note::
            SPECFEM3D/3D_GLOBE versions must overwrite this function

        .. note::
            If the glob returns an  empty list, this function exits the
            workflow because filenames should not be empty is they're being
            queried

        :rtype: list
        :return: list of data filenames
        """
        assert(choice in ["obs", "syn", "adj"]), \
            f"choice must be: 'obs', 'syn' or 'adj'"

        if self.components:
            comp_glob = f"[{self.components}]"  # 'NEZ' -> '[NEZ]' for wildcard
        else:
            comp_glob = "?"
        data_wildcard = self.data_wildcard(comp=comp_glob)
        file_glob = os.path.join(self.cwd, "traces", choice, data_wildcard)
        filenames = glob(file_glob)

        if not filenames:
            logger.critical(
                msg.cli("The property `solver.data_filenames`, used to search "
                        "for waveform files, is empty and should not be. "
                        "Please check solver parameters: ",
                        items=[f"failed wildcard: {file_glob}"],
                        header="data filenames error", border="=")
            )
            sys.exit(-1)

        return filenames

    @property
    def model_databases(self):
        """
        The location of model inputs and outputs as defined by SPECFEM2D.
        This is RELATIVE to a SPECFEM2D working directory.

         .. note::
            This path is SPECFEM version dependent so SPECFEM3D/3D_GLOBE
            versions must overwrite this function

        :rtype: str
        :return: path where SPECFEM2D database files are stored, relative to
            `solver.cwd`
        """
        return "DATA"

    @property
    def model_files(self):
        """
        Return a list of paths to model files that match the internal parameter
        list. Used to generate model vectors of the same length as gradients.

        :rtype: list
        :return: a list of full paths to model files that matches the internal
            list of solver parameters
        """
        _model_files = []
        for par in self._parameters:
            _model_files += glob(os.path.join(self.path.mainsolver,
                                              self.model_databases,
                                              self.model_wildcard(par=par))
                                              )
        return _model_files

    @property
    def kernel_databases(self):
        """
        The location of kernel inputs and outputs as defined by SPECFEM2D
        This is RELATIVE to a SPECFEM2D working directory.

         .. note::
            This path is SPECFEM version dependent so SPECFEM3D/3D_GLOBE
            versions must overwrite this function

        :rtype: str
        :return: path where SPECFEM2D database files are stored, relative to
            `solver.cwd`
        """
        return "OUTPUT_FILES"

    def forward_simulation(self, save_traces=False,
                           export_traces=False, save_forward_arrays=False,
                           flag_save_forward=True, **kwargs):
        """
        Wrapper for SPECFEM binaries: 'xmeshfem?D' 'xgenerate_databases',
                                      'xspecfem?D'

        Calls SPECFEM2D forward solver, exports solver outputs to traces dir

         .. note::
            SPECFEM3D/3D_GLOBE versions must overwrite this function

        :type save_traces: str
        :param save_traces: move files from their native SPECFEM output location
            to another directory. This is used to move output waveforms to
            'traces/obs' or 'traces/syn' so that SeisFlows knows where to look
            for them, and so that SPECFEM doesn't overwrite existing files
            during subsequent forward simulations
        :type export_traces: str
        :param export_traces: export traces from the scratch directory to a more
            permanent storage location. i.e., copy files from their original
            location
        :type save_forward_arrays: str
        :param save_forward_arrays: relative path (relative to 
            /scratch/solver/<source_name>/<model_database>) to move the forward 
            arrays which are used for adjoint simulations. Mainly used for 
            ambient noise adjoint tomography which requires multiple forward 
            simulations prior to adjoint simulations, putting forward arrays 
            at the risk of overwrite. Normal Users can leave this default.
        :type flag_save_forward: bool
        :param flag_save_forward: whether to turn on the flag for saving the 
            forward arrays which are used for adjoint simulations. Not required 
            if only running forward simulations.
        """
        unix.cd(self.cwd)
        setpar(key="SIMULATION_TYPE", val="1", file="DATA/Par_file")
        setpar(key="SAVE_FORWARD", val=f".{str(flag_save_forward).lower()}.",
               file="DATA/Par_file")

        # Calling subprocess.run() for each of the binary executables listed
        for exc in self._fwd_simulation_executables:
            # e.g., fwd_mesher.log
            stdout = f"fwd_{self._exc2log(exc)}.log"
            self._run_binary(executable=exc, stdout=stdout)

        # Error check to ensure that mesher and solver have been run succesfully
        _solv = bool(glob(os.path.join("OUTPUT_FILES", self.data_wildcard())))
        if not _solv:
            logger.critical(msg.cli(f"solver failed to produce expected files",
                            header="external solver error", border="="))
            sys.exit(-1)

        # Work around SPECFEM's version dependent file names
        if self.syn_data_format.upper() == "SU":
            for tag in ["d", "v", "a", "p"]:
                unix.rename(old=f"single_{tag}.su", new="single.su",
                            names=glob(os.path.join("OUTPUT_FILES", "*.su")))
        # Exporting traces to disk (output/) for more permanent storage
        if export_traces:
            if not os.path.exists(export_traces):
                unix.mkdir(export_traces)
            unix.cp(
                src=glob(os.path.join("OUTPUT_FILES", self.data_wildcard())),
                dst=export_traces
            )
        # Save traces somewhere else in the scratch/ directory for easier access
        if save_traces:
            if not os.path.exists(save_traces):
                unix.mkdir(save_traces)
            unix.mv(
                src=glob(os.path.join("OUTPUT_FILES", self.data_wildcard())),
                dst=save_traces
            )
        # Save forward arrays to disk for later adjoint simulations. This is
        # primarily used for ambient noise adjoint tomography when other
        # forward simulations are required prior to the adjoint simulation,
        # which would overwrite existing forward arrays
        if save_forward_arrays:
            # scratch/solver/<source_name>/<save_forward_arrays>
            save_forward_arrays = os.path.join(self.cwd, save_forward_arrays)
            if not os.path.exists(save_forward_arrays):
                unix.mkdir(save_forward_arrays)
            for glob_key in [self._forward_array_wildcard, 
                             self._absorb_wildcard]:                                   
                unix.mv(src=glob(os.path.join(self.model_databases, glob_key)),
                        dst=save_forward_arrays)

        # Delete unncessary visualization files which may be large. This is 
        # only relevant for SPECFEM3D/3D_GLOBE, but will not throw errors for 2D
        if self.prune_scratch:
            logger.debug("prune scratch: removing '*.vt?' files from database")
            unix.rm(glob(os.path.join(self.model_databases, 
                                      "proc??????_*.vt?")))

        logger.info(f"FINISH FORWARD SIMULATION: {self.source_name}")

    def adjoint_simulation(self, save_kernels=False, export_kernels=False,
                           load_forward_arrays=False, 
                           del_loaded_forward_arrays=False, **kwargs):
        """
        Wrapper for SPECFEM binary 'xspecfem?D'

        Calls SPECFEM2D adjoint solver, creates the `SEM` folder with adjoint
        traces which is required by the adjoint solver. Renames kernels
        after they have been created from 'alpha' and 'beta' to 'vp' and 'vs',
        respectively.

         .. note::
            SPECFEM3D/3D_GLOBE versions must overwrite this function

        :type save_kernels: str
        :param save_kernels: move the kernels from their native SPECFEM output
            location to another path. This is used to move kernels to another
            SeisFlows scratch directory so that they are discoverable by
            other modules. The typical location they are moved to is
            path_eval_grad
        :type export_kernels: str
        :param export_kernels: export/copy/save kernels from the scratch
            directory to a more permanent storage location. i.e., copy files
            from their original location. Note that kernel file sizes are LARGE,
            so exporting kernels can lead to massive storage requirements.
        :type load_forward_arrays: str
        :param load_forward_arrays: relative path (relative to solver.cwd) to 
            load previously generated forward arrays which are used for adjoint 
            simulations. Mainly used for ambient noise adjoint tomography. Will 
            OVERWRITE any forward array files already located in the database 
            directory.
        :type del_loaded_forward_arrays: bool
        :param del_loaded_forward_arrays: only used if `load_forward_arrays` is
            set. After adjoint simulation completes nominally, delete the 
            forward arrays that were used to run the adjoint simulation to 
            save space. Usually
        """
        unix.cd(self.cwd)

        setpar(key="SIMULATION_TYPE", val="3", file="DATA/Par_file")
        setpar(key="SAVE_FORWARD", val=".false.", file="DATA/Par_file")

        unix.rm("SEM")
        unix.ln("traces/adj", "SEM")

        # Pre-load forward arrays if necessary
        if load_forward_arrays:
            logger.info(f"loading forward arrays: '{load_forward_arrays}'")
            
            # scratch/solver/<source_name>/<load_forward_arrays>
            load_forward_arrays = os.path.join(self.cwd, load_forward_arrays)

            # Few sanity checks to make sure something is actually loaded
            if not os.path.exists(load_forward_arrays):
                logger.critical(f"forward arrays not found: "
                                f"{load_forward_arrays}")
                sys.exit(-1)
            if not glob(os.path.join(load_forward_arrays, "*")):
                logger.critical(f"forward array's empty {load_forward_arrays}")
                sys.exit(-1)
            
            # 'cp' command will OVERWRITE existing forward arrays in the dir.
            for fwd_arr in glob(os.path.join(load_forward_arrays, "*")):
                fid = os.path.basename(fwd_arr)
                unix.cp(src=fwd_arr, dst=os.path.join(self.cwd, 
                                                      self.model_databases, 
                                                      fid)
                        )

        # Calling subprocess.run() for each of the binary executables listed
        for exc in self._adj_simulation_executables:
            # e.g., adj_solver.log
            stdout = f"adj_{self._exc2log(exc)}.log"
            logger.info(f"running SPECFEM executable {exc}, log to '{stdout}'")
            self._run_binary(executable=exc, stdout=stdout)

        # Rename 'alpha' -> 'vp' and 'beta' -> 'vs' for consistency. 
        # Wait a few seconds before doing this to avoid race condition of
        # kernel file creation and renaming
        self._rename_kernel_parameters()

        # Kernel export and saving must take place within the kernel directory
        unix.cd(os.path.join(self.cwd, self.kernel_databases))

        # Export kernels: copy them to some external directory for storage
        if export_kernels:
            unix.mkdir(export_kernels)
            for par in self._parameters:
                kernel_files = glob(self.model_wildcard(par=par, kernel=True))
                if kernel_files:
                    logger.debug(f"copying '{par}' kernels to {export_kernels}")
                    unix.cp(src=kernel_files, dst=export_kernels)
                else:
                    logger.warning(f"no kernel files for '{par}', cant export")

        # Save kernels: move kernels to an internal directory for later steps
        # so they don't get overwritten by future adjoint simulations
        if save_kernels:
            unix.mkdir(save_kernels)
            for par in self._parameters:
                kernel_files = glob(self.model_wildcard(par=par, kernel=True))
                if kernel_files:
                    logger.debug(f"moving '{par}' kernels to {save_kernels}")
                    unix.mv(src=kernel_files, dst=save_kernels)
                else:
                    logger.critical(f"no kernel files found for '{par}', "
                                    f"please check adjoint solver log for "
                                    f"{self.source_name}")
                    sys.exit(-1)

        # Working around fact that `absorb_buffer` files have diff naming w.r.t
        # SPECFEM3D. Will also remove `save_forward_arrays` to free up space
        # since we no longer need these
        if self.prune_scratch:                                                   
            for glob_key in [self._forward_array_wildcard, 
                             self._absorb_wildcard]:
                logger.debug(f"prune scratch: removing '{glob_key}' files"
                             f"from database ")                                  
                unix.rm(glob(os.path.join(self.model_databases, glob_key)))

        if load_forward_arrays and del_loaded_forward_arrays:
            logger.debug(f"removing loaded forward arrays: "
                         f"{load_forward_arrays}")
            unix.rm(load_forward_arrays)

        logger.info(f"FINISH ADJOINT SIMULATION: {self.source_name}")

    def _rename_kernel_parameters(self):
        """
        Rename kernels to work w/ conflicting name conventions.
        - alpha -> vp
        - beta -> vs
        
        Performed directly inside the directory so that rename won't affect
        any strings in the full path. Deals with both SPECFEM3D and 3D_GLOBE.
        GLOBE version adds in the 'reg?' tag that needs to be considered.

        Kept as a separate function so it can be called outside the adjoint
        simulation task for debugging purposes.
        """
        unix.cd(os.path.join(self.cwd, self.kernel_databases))

        for tag in ["alpha", "alpha[hv]", "reg?_alpha", "reg?_alpha[hv]"]:
            names = glob(self.model_wildcard(par=tag, kernel=True))
            if names:
                logger.info(f"renaming {len(names)} kernels: '{tag}' -> 'vp'")
                unix.rename(old="alpha", new="vp", names=names)

        for tag in ["beta", "beta[hv]", "reg?_beta", "reg?_beta[hv]"]:
            names = glob(self.model_wildcard(par=tag, kernel=True))
            if names:
                logger.info(f"renaming {len(names)} kernels: '{tag}' -> 'vs'")
                unix.rename(old="beta", new="vs", names=names)

    def combine(self, input_paths, output_path, parameters=None):
        """
        Wrapper for 'xcombine_sem'.
        Sums kernels from individual source contributions to create gradient.

        .. note::
            The binary xcombine_sem simply sums matching databases

        .. note::
            It is ASSUMED that this function is being called by
            system.run(single=True) so that we can use the main solver
            directory to perform the kernel summation task

        :type input_paths: list
        :param input_paths: list of paths to directories containing binary
            files to be combined
        :type output_path: str
        :param output_path: path to export the outputs of xcombine_sem
        :type parameters: list
        :param parameters: optional list of parameters,
            defaults to `self._parameters`
        """
        unix.cd(self.cwd)

        if parameters is None:
            parameters = self._parameters

        if not os.path.exists(output_path):
            unix.mkdir(output_path)

        # Write the source names into the kernel paths file for SEM/ directory
        with open("kernel_paths", "w") as f:
            for input_path in input_paths:
                f.write(f"{input_path}\n")

        # Call on xcombine_sem to combine kernels into a single file
        for name in parameters:
            # e.g.: mpiexec bin/xcombine_sem alpha_kernel kernel_paths output/
            exc = f"bin/xcombine_sem {name} kernel_paths {output_path}"
            # e.g., smooth_vp.log
            stdout = f"{self._exc2log(exc)}_{name}.log"
            self._run_binary(executable=exc, stdout=stdout, with_mpi=True)

    def smooth(self, input_path, output_path, parameters=None, span_h=None,
               span_v=None, use_gpu=False):
        """
        Wrapper for SPECFEM binary: xsmooth_sem
        Smooths kernels by convolving them with a 3D Gaussian

        .. note::
            It is ASSUMED that this function is being called by
            system.run(single=True) so that we can use the main solver
            directory to perform the kernel smooth task

        :type input_path: str
        :param input_path: path to data
        :type output_path: str
        :param output_path: path to export the outputs of xcombine_sem
        :type parameters: list
        :param parameters: optional list of parameters,
            defaults to `self._parameters`
        :type span_h: float
        :param span_h: horizontal smoothing length in meters
        :type span_v: float
        :param span_v: vertical smoothing length in meters
        :type use_gpu: bool
        :param use_gpu: whether to use GPU acceleration for smoothing. Requires
            GPU compiled binaries and GPU compute node.
        """
        unix.cd(self.cwd)

        # Assign some default parameters from class attributes if not given
        if parameters is None:
            parameters = self._parameters
        if span_h is None:
            span_h = self.smooth_h
        if span_v is None:
            span_v = self.smooth_v

        logger.debug(f"smoothing {parameters} with horizontal Gaussian "
                     f"{span_h}m and vertical Gaussian {span_v}m")

        if not os.path.exists(output_path):
            unix.mkdir(output_path)

        # Ensure trailing '/' character, required by xsmooth_sem
        input_path = os.path.join(input_path, "")
        output_path = os.path.join(output_path, "")
        if use_gpu:
            use_gpu = ".true"
        else:
            use_gpu = ".false"
        # mpiexec ./bin/xsmooth_sem SMOOTH_H SMOOTH_V name input output use_gpu
        for name in parameters:
            exc = (f"bin/xsmooth_sem {str(span_h)} {str(span_v)} {name}_kernel "
                   f"{input_path} {output_path} {use_gpu}")
            # e.g., combine_vs.log
            stdout = f"{self._exc2log(exc)}_{name}.log"
            self._run_binary(executable=exc, stdout=stdout, with_mpi=True)

        # Rename output files to remove the '_smooth' suffix which SeisFlows
        # will not recognize
        files = glob(os.path.join(output_path, "*"))
        unix.rename(old="_smooth", new="", names=files)

    def _run_binary(self, executable, stdout="solver.log", with_mpi=True):
        """
        Calls MPI solver executable to run solver binaries, used by individual
        processes to run the solver on system. If the external solver returns a
        non-zero exit code (failure), this function will return a negative
        boolean.

        .. note::
            This function ASSUMES it is being run from a SPECFEM working
            directory, i.e., that the executables are located in ./bin/

        .. note::
            This is essentially an error-catching wrapper of subprocess.run()

        :type executable: str
        :param executable: executable function to call. May or may not start
            E.g., acceptable calls for the solver would './bin/xspecfem2D'.
            Also accepts additional command line arguments such as:
            'xcombine_sem alpha_kernel kernel_paths...'
        :type stdout: str
        :param stdout: where to redirect stdout
        :type with_mpi: bool
        :param with_mpi: If `mpiexec` is given, use MPI to run the executable.
            Some executables (e.g., combine_vol_data_vtk) must be run in
            serial so this flag allows them to turn off MPI running.
        :raises SystemExit: If external numerical solver return any failure
            code while running
        """
        # Executable may come with additional sub arguments, we only need to
        # check that the actually executable exists
        if not unix.which(executable.split(" ")[0]):
            logger.critical(msg.cli(f"executable '{executable}' does not exist",
                            header="external solver error", border="="))
            sys.exit(-1)

        # Prepend with `mpiexec` if we are running with MPI
        # looks something like: `mpirun -n 4 ./bin/xspecfem2d`
        if self._mpiexec and with_mpi:
            executable = f"{self._mpiexec} -n {self.nproc} {executable}"
        logger.debug(f"running executable with cmd: '{executable}'")

        try:
            with open(stdout, "w") as f:
                subprocess.run(executable, shell=True, check=True, stdout=f,
                               stderr=f)
        except (subprocess.CalledProcessError, OSError) as e:
            logger.critical(
                msg.cli("The external numerical solver has returned a "
                        "nonzero exit code (failure). Consider stopping any "
                        "currently running jobs to avoid wasted "
                        "computational resources. Check 'scratch/solver/"
                        f"mainsolver/{stdout}' for the solvers stdout log "
                        "message. The failing command and error message are:",
                        items=[f"exc: {executable}", f"err: {e}"],
                        header="external solver error",
                        border="=")
            )
            sys.exit(-1)

    @staticmethod
    def _exc2log(exc):
        """
        Very simple conversion utility to get log file names based on binaries.
        e.g., binary 'xspecfem2D' will return 'solver'. Helps keep log file
        naming consistent and generalizable

        TODO add a check here to see if the log file exists, and then use
            `number_fid` to increment so that we keep all the output logs

        :type exc: str
        :param exc: specfem executable, e.g., xspecfem2D, xgenerate_databases
        :rtype: str
        :return: logfile name that matches executable name
        """
        convert_dict = {"specfem": "solver", "meshfem": "mesher",
                        "generate_databases": "mesher", "smooth": "smooth",
                        "combine": "combine"}
        for key, val in convert_dict.items():
            if key in exc:
                return val
        else:
            return "logger"

    def import_model(self, path_model):
        """
        Copy files from given `path_model` into the current working directory
        model database. Used for grabbing starting models (e.g., MODEL_INIT)
        and models that have been perturbed by the optimization library.

        :type path_model: str
        :param path_model: path to an existing starting model
        """
        assert(os.path.exists(path_model)), f"model {path_model} does not exist"
        unix.cd(self.cwd)

        # Copy the model files (ex: proc000023_vp.bin ...) into database dir
        src = glob(os.path.join(path_model, f"*{self._ext}"))
        dst = os.path.join(self.cwd, self.model_databases, "")
        unix.cp(src, dst)

    def _initialize_working_directories(self, max_workers=None):
        """
        Serial or parallel task used to initialize working directories for
        each of the available sources

        :type max_workers: int
        :param max_workers: number of concurrent tasks to use when creating 
            working directories. Defaults to using all available cores on 
            the machine since this is a lightweight task
        """
        if max_workers is None:
            max_workers = unix.nproc() - 1  # use all available cores

        # Full path each source in the scratch directory for directories that
        # do not exist, otherwise this function gets skipped
        source_paths = [os.path.join(self.path.scratch, source_name)
                        for source_name in self.source_names]
        source_paths = [p for p in source_paths if not os.path.exists(p)]

        if source_paths:
            logger.info(f"initializing {self.ntask} solver directories")

        if max_workers > 1:
            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                futures = [
                    executor.submit(self._initialize_working_directory, cwd)
                    for cwd in source_paths
                ]
            wait(futures)
            # If any of the jobs, calling the result will raise the Exception
            for future in futures:
                try:
                    future.result()
                except Exception as e:
                    logger.critical(f"directory initialization error: {e}")
                    sys.exit(-1)
        else:
            for source_name in self.source_names:
                cwd = os.path.join(self.path.scratch, source_name)
                if os.path.exists(cwd):
                    continue
                self._initialize_working_directory(cwd=cwd)

    def _initialize_working_directory(self, cwd=None):
        """
        Creates scratch directory structure expected by SPECFEM
        (i.e., bin, DATA, OUTPUT_FILES). Copies executables (bin) and
        input data (DATA) directories, prepares simulation input files.

        Each directory will act as completely independent Specfem working dir.
        This allows for embarrassing parallelization while avoiding the need
        for intra-directory communications, at the cost of temporary disk space.

        .. note::
            path to binary executables must be supplied by user as SeisFlows has
            no mechanism for automatically compiling from source code.

        :type cwd: str
        :param cwd: optional scratch working directory to intialize. If None,
            will set based on current running seisflows task (self.taskid)
        """
        # Define a constant list of required SPECFEM dir structure, relative cwd
        _required_structure = {"bin", "DATA", "OUTPUT_FILES", "traces/obs", 
                               "traces/syn", "traces/adj", self.model_databases,
                               self.kernel_databases}

        # Allow this function to be called on system or in serial
        if cwd is None:
            cwd = self.cwd
            source_name = self.source_name
        else:
            source_name = os.path.basename(cwd)

        _idx = self.source_names.index(source_name)
        logger.debug(f"source {_idx}: {source_name}")
        # Starting from a fresh working directory
        unix.rm(cwd)
        unix.mkdir(cwd)
        for dir_ in _required_structure:
            unix.mkdir(os.path.join(cwd, dir_))

        # Copy existing SPECFEM exectuables into the bin/ directory
        src = glob(os.path.join(self.path.specfem_bin, "*"))
        dst = os.path.join(cwd, "bin", "")
        unix.cp(src, dst)

        # Copy in all input DATA/ file that are not '{source_prefix}*'
        src = glob(os.path.join(self.path.specfem_data, "*"))
        src = [_ for _ in src if not
               os.path.basename(_).startswith(self.source_prefix)]
        dst = os.path.join(cwd, "DATA", "")
        unix.cp(src, dst)

        # Symlink event source specifically, only retain source prefix
        src = os.path.join(self.path.specfem_data,
                           f"{self.source_prefix}_{source_name}")
        dst = os.path.join(cwd, "DATA", self.source_prefix)
        unix.ln(src, dst)

        # Symlink TaskID==0 as mainsolver in solver directory for convenience
        if self.source_names.index(source_name) == 0:
            if not os.path.exists(self.path.mainsolver):
                logger.debug(f"linking source '{source_name}' as 'mainsolver'")
                unix.ln(cwd, self.path.mainsolver)

    def _export_starting_models(self, parameters=None):
        """
        Export the initial and target models to the SeisFlows output/ directory.

        :type parameters: list
        :param parameters: list of parameters to export. If None, will default
            to `self._parameters`
        """
        if parameters is None:
            parameters = self._parameters

        # Export the initial and target models to the SeisFlows output directory
        for name, model in zip(["MODEL_INIT", "MODEL_TRUE"],
                               [self.path.model_init, self.path.model_true]):
            # Skip over if user has not provided model path (e.g., real data
            # inversion will not have `model_true`)
            if not model:
                continue
            dst = os.path.join(self.path.output, name, "")
            if not os.path.exists(dst):
                unix.mkdir(dst)
            for par in parameters:
                src = glob(os.path.join(model, f"*{par}{self._ext}"))
                unix.cp(src, dst)

    def make_output_vtk_files(self, input_path, output_path=None, 
                              parameters=None, hi_res=False, tag=None, 
                              kernel=False):
        """
        A warpper on `combine_vol_data_vtk()` that automatically tries to 
        generate .vtk files using the SPECFEM binary xcombine_vol_data_vtk, 
        and rename the output files to not be so generic. Files will be stored
        in the `output_path` directory, and will be named based on the `tag` 
        unless overwritten by the User.

        :type input_path: str
        :param input_path: path to database files to be summed.
        :type output_path: strs
        :param output_path: path to export the outputs of the binary
        :type parameters: list
        :param parameters: optional list of parameters, defaults to 
            `self._parameters` if None provided (e.g., ['vp', 'vs'])
        :type tag: str
        :param tag: optional tag to rename output vtk files. If not provided,
            will use the name of the directory holding the files
        :type kernel: bool
        :param kernel: whether the files being converted are kernel files or
            model files. This changes the file naming convention
        :type hi_res: bool
        :param hi_res: Set the high resolution flag to 1 or True, which will
            generate .vtk files with data at EACH GLL point, rather than at each
            nodal vertex. These files are LARGE, and we discourage using
            `hi_res`==True unless you know you want these files.
        """
        # Check that we are using the correct Solver type (3D, 3D_GLOBE)
        if not hasattr(self, "combine_vol_data_vtk"):
            logger.warning("solver does not have the capability to generate "
                           "VTK files, skipping")
            return
        elif not os.path.exists(os.path.join(self.path.specfem_bin, 
                                             "xcombine_vol_data_vtk")):
            logger.warning("solver does not have the required binary "
                           "'xcombine_vol_data_vtk', please compile this "
                           "binary to make VTK files. Skipping ")
            return
        
        # Set some default parameters if not overwritten by User
        if not output_path:
            output_path = os.path.join(self.path._solver_output, "VTK")

        # Determine how to rename files after creation
        if not tag:
            tag = os.path.basename(input_path)
        
        # Set which parameters will be made into VTK files. Do not re-create
        # files that already exist. Add '_kernel' for kernel and gradient files
        if not parameters:
            parameters = []
            for par in self._parameters:
                if kernel:
                    par = f"{par}_kernel"
                # Strip reg?_ from SPECFEM3D_GLOBE parameter names
                # e.g., reg1_vsh -> vsh
                if self._regions:
                    par = par[5:]
                # File naming should follow a standard format that we validate
                check = glob(os.path.join(output_path, f"{tag}*{par}.vtk"))
                if not check:
                    parameters.append(par)
                else:
                    continue
        if not parameters:
            return
        
        self.combine_vol_data_vtk(
            input_path=input_path, output_path=output_path, 
            parameters=parameters, hi_res=hi_res
            )
        # Wait for the process to finish before trying to rename files
        time.sleep(5 * len(parameters))
        
            # SPECFEM3D_GLOBE will tag files based on region
        for par in parameters:
            if self._regions is not None:
                for region in self._regions:
                    src = os.path.join(output_path, f"reg_{region}_{par}.vtk")
                    dst = os.path.join(
                        output_path, f"{tag}_reg_{region}_{par}.vtk")
                    if os.path.exists(src):
                        unix.mv(src, dst)
            else:
                src = os.path.join(output_path, f"{par}.vtk")
                dst = os.path.join(output_path, f"{tag}_{par}.vtk")
                if os.path.exists(src):
                    unix.mv(src, dst)

    def finalize(self):
        """
        General finalization procedures for SPECFEM-based solver activities
        """
        # Generate VTK files for everything in output path
        if self._export_vtk:
            for name in ["MODEL", "GRADIENT"]:
                for fid in glob(os.path.join(self.path.output, f"{name}_*")):
                    self.make_output_vtk_files(
                        input_path=fid, kernel=bool(name=="GRADIENT")
                        )
        

